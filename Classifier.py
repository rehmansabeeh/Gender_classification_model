# -*- coding: utf-8 -*-
"""21100016_21100025_21100186_21100146_21100105.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11zDUo3RLR_WiBz81auP4d3JYCdn8d35l

# Project Phase II: Gender & Speaker Recognition
- Hayat Awais Malik (21100016)
- Muhammad Junaid (21100025)
- Muhammad Sabeeh Rehman (21100186)
- Muhammad Shehryaar Sharif (21100146)
- Rabeeya Hamid (21100105)
"""

!pip install python_speech_features

import python_speech_features as mfcc
from scipy.io.wavfile import read
import numpy as np
import os
import matplotlib.pyplot as plt
import time
from tqdm.notebook import tqdm
from sklearn.metrics import accuracy_score,confusion_matrix,classification_report

"""## Downloading Dataset"""

!gdown --id 1GbfEJ4JnTGthMs4RP5z17BrCQm6YxtEG

!unzip /content/LUMS_FALL2020_PROJECT_DATA.zip -d /content/dataset_project

"""## File Reading & Feature Extraction"""

# Function to extract audio features
def get_MFCC(audio, sr):
    features = mfcc.mfcc(audio, sr, 0.025, 0.01, 13, appendEnergy = True)
    return np.mean(features, axis=0)

sr, audio = read('/content/dataset_project/Gender_Recognition/Test/SPK083_M/1.wav')
features = get_MFCC(audio, sr)

Gender_Recognition_path='/content/dataset_project/Gender_Recognition/'
Speaker_Recognition_path='/content/dataset_project/Speaker_Recognition/'

Train_data_GR=[]
Test_data_GR=[]
Valid_data_GR=[]

labels_Train_data_GR=[]
labels_Test_data_GR=[]
labels_Valid_data_GR=[]

Train_data_SR=[]
Test_data_SR=[]
Valid_data_SR=[]

labels_Train_data_SR=[]
labels_Test_data_SR=[]
labels_Valid_data_SR=[]

Data_types_GR = os.listdir(Gender_Recognition_path)
Data_types_SR = os.listdir(Speaker_Recognition_path)

train_folder_GR=os.listdir(Gender_Recognition_path+"/"+Data_types_GR[2])
test_folder_GR=os.listdir(Gender_Recognition_path+"/"+Data_types_GR[1])
valid_folder_GR=os.listdir(Gender_Recognition_path+"/"+Data_types_GR[0])

train_folder_SR=os.listdir(Speaker_Recognition_path+"/"+Data_types_SR[2])
test_folder_SR=os.listdir(Speaker_Recognition_path+"/"+Data_types_SR[1])
valid_folder_SR=os.listdir(Speaker_Recognition_path+"/"+Data_types_SR[0])

# print(Data_types_SR)
# print(Data_types_GR)

#training features for Gender Recognition
for folder in train_folder_GR:
  if folder[-1] =="M":
    [labels_Train_data_GR.append(0) for i in range(10)]
  else:
    [labels_Train_data_GR.append(1) for i in range(10)]
  
  vn_folder=os.listdir(Gender_Recognition_path+"/"+Data_types_GR[2]+"/"+folder)
  for vn in vn_folder:
    sr, audio = read(Gender_Recognition_path+"/"+Data_types_GR[2]+"/"+folder+"/"+vn)
    features = get_MFCC(audio, sr)
    # print (features.shape)
    Train_data_GR.append(features)

#testing features for Gender Recognition
for folder in test_folder_GR:
  if folder[-1] =="M":
    [labels_Test_data_GR.append(0) for i in range(10)]
  else:
    [labels_Test_data_GR.append(1) for i in range(10)]
    
  vn_folder=os.listdir(Gender_Recognition_path+"/"+Data_types_GR[1]+"/"+folder)
  for vn in vn_folder:
    sr, audio = read(Gender_Recognition_path+"/"+Data_types_GR[1]+"/"+folder+"/"+vn)
    features = get_MFCC(audio, sr)
    # print (features.shape)
    Test_data_GR.append(features)

#Validation features for Gender Recognition
for folder in valid_folder_GR:
  if folder[-1] =="M":
    [labels_Valid_data_GR.append(0) for i in range(10)]
  else:
    [labels_Valid_data_GR.append(1) for i in range(10)]

  vn_folder=os.listdir(Gender_Recognition_path+"/"+Data_types_GR[0]+"/"+folder)
  for vn in vn_folder:
    sr, audio = read(Gender_Recognition_path+"/"+Data_types_GR[0]+"/"+folder+"/"+vn)
    features = get_MFCC(audio, sr)
    # print (features.shape)
    Valid_data_GR.append(features)


#training features for Speaker Recognition
for folder in train_folder_SR:
  [labels_Train_data_SR.append(int(folder[3:6]) - 1) for i in range(6)]
  vn_folder=os.listdir(Speaker_Recognition_path+"/"+Data_types_SR[2]+"/"+folder)
  for vn in vn_folder:
    sr, audio = read(Speaker_Recognition_path+"/"+Data_types_SR[2]+"/"+folder+"/"+vn)
    features = get_MFCC(audio, sr)
    # print (features.shape)
    Train_data_SR.append(features)

#testing features for Speaker Recognition
for folder in test_folder_SR:
  [labels_Test_data_SR.append(int(folder[3:6]) - 1) for i in range(2)]
  vn_folder=os.listdir(Speaker_Recognition_path+"/"+Data_types_SR[1]+"/"+folder)
  for vn in vn_folder:
    sr, audio = read(Speaker_Recognition_path+"/"+Data_types_SR[1]+"/"+folder+"/"+vn)
    features = get_MFCC(audio, sr)
    # print (features.shape)
    Test_data_SR.append(features)

#Validation features for Speaker Recognition
for folder in valid_folder_SR:
  [labels_Valid_data_SR.append(int(folder[3:6]) - 1) for i in range(2)]
  vn_folder=os.listdir(Speaker_Recognition_path+"/"+Data_types_SR[0]+"/"+folder)
  for vn in vn_folder:
    sr, audio = read(Speaker_Recognition_path+"/"+Data_types_SR[0]+"/"+folder+"/"+vn)
    features = get_MFCC(audio, sr)
    # print (features.shape)
    Valid_data_SR.append(features)


Train_data_GR=np.array(Train_data_GR)
Train_data_SR=np.array(Train_data_SR)

Test_data_GR=np.array(Test_data_GR)
Test_data_SR=np.array(Test_data_SR)

Valid_data_GR=np.array(Valid_data_GR)
Valid_data_SR=np.array(Valid_data_SR)



labels_Train_data_GR=np.array(labels_Train_data_GR)
labels_Train_data_SR=np.array(labels_Train_data_SR)

labels_Test_data_GR=np.array(labels_Test_data_GR)
labels_Test_data_SR=np.array(labels_Test_data_SR)

labels_Valid_data_GR=np.array(labels_Valid_data_GR)
labels_Valid_data_SR=np.array(labels_Valid_data_SR)


# train_entries_positive = os.listdir('/content/dataset_ass4/Dataset/train/pos')
# test_entries_negative = os.listdir('/content/dataset_ass4/Dataset/test/neg')
# test_entries_positive = os.listdir('/content/dataset_ass4/Dataset/test/pos')

"""## Implementation Functions

### Helper Functions

#### Adding Bias
*   Parameter: feature vector; size: m x n-1 matrix
*   Returns: feature vector; size: m x n matrix
"""

def append_bias(features):
  features = np.insert(features, 0, 1, axis=1)
  return features

"""#### Creating weights
*   Parameter: scalars n and c
*   Returns: weight vector; size: n x c matrix
"""

def making_weights(n, c):
  weights = []
  for i in range(c):
    weights_class = np.ones(n) 
    weights.append(weights_class)
  weights = np.array(weights)
  weights = weights.T
  return weights

"""#### Hypothesis Function
*   Parameter: feature vector; size: m x n , weight vector; size: n x c
*   Returns: weight vector; size: m x c matrix
"""

def h_x(features, weights):
  hypothesis_data = np.matmul(features, weights)
  return hypothesis_data

"""#### Softmax Function

*   Parameter: hypothesis data (with bias) size; m x c matrix
*   Returns: probability distribution over data; size: m x c matrix


"""

def soft_max(hyp_data):
  shape = hyp_data.shape
  m = shape[0]
  c = shape[1]
  softmax = np.zeros(shape)
  for i in range(0, m):
    sum = 0
    for j in range(0, c): 
      softmax[i,j] = np.exp(hyp_data[i,j])
      sum = sum + softmax[i,j]
    softmax[i,:] = softmax[i,:]/sum
  return softmax

"""#### Prediction Function

*   Parameter: softmax array size; m x c matrix
*   Returns: predicted array; size: m x 1 matrix

"""

def prediction(softmax_arr):
  return np.argmax(softmax_arr,axis=1)

"""#### Cross Entropy Loss

*   Parameters: 

  *   prob_dist: size= m x c 
  *   labels: size= m x 1 

*   Returns: total loss over , size: scalar
"""

def ce_loss(labels, prob_dist):
  loss = 0
  shape = prob_dist.shape
  m = shape[0]
  c = shape[1]
  for i in range(0, m):
    for j in range(0, c):
      if labels[i] == j:
        loss = loss - np.log(prob_dist[i,j])
  return loss/m

"""#### Calculate Weight Deltas
- parameters:
    - features: feature vector (m, n)
    - labels: Label vector (m, 1)
    - prob_dist: probability distribution (m, c)
    - alpha: scalar learning rate
- returns: deltas to be updated (n, c)
"""

def compute_weights(features, labels, prob_dist, alpha):
  func1 = np.zeros(prob_dist.shape)
  for i, row in enumerate(prob_dist):
    func1[i, labels[i]] = 1
  loss_der = np.zeros((features.shape[1], prob_dist.shape[1]))
  for c in range(prob_dist.shape[1]):
    for n in range(features.shape[1]):
      prob_diff = (func1[:, c] - prob_dist[:, c])
      sum_x=0
      for m in range(features.shape[0]):
        sum_x += features[m,n]* prob_diff[m]
      loss_der[n, c] = sum_x
  delta = (-alpha/features.shape[0]) * loss_der
  return delta

"""#### Gradient Descent
- parameters:
    - n_epoch: scalar
    - alpha: scalar
    - features: feature vector (m, n)
    - labels: label vector (m, 1)
    - weights: model weights (m, c)
- returns:
    - j: list representing training loss of length n_epoch
    - weights: learned model weights (m, c)
"""

def grad_descent(n_epoch,alpha,features,labels,weights):
  j=[]
  for epoch in tqdm(range(n_epoch)):
    z=h_x(features,weights)
    activations=soft_max(z)
    deltas=compute_weights(features,labels,activations,alpha)
    weights -=  deltas
    j.append(ce_loss(labels,activations))
  return j, weights

"""### Multinomial LR Training
- parameters:
  - features: feature vector (m, n)
  - labels: label vector (m, 1)
  - n_epoch: scalar
  - alpha: scalar
- returns:
  - loss: list of length n_epoch
  - weights: weight vector (m, c)
"""

def multinomial_lr_fit(features, labels, n_epoch=100, alpha=0.001):
  features = append_bias(features)
  m, n, c = features.shape[0], features.shape[1], len(set(labels))
  weights = making_weights(n, c)
  loss, weights = grad_descent(n_epoch, alpha, features, labels, weights)
  return loss, weights

"""### Multinomial LR Evaluation
- parameters:
  - features: feature vector (m, n)
  - labels: label vector (m, 1)
  - weights: weight vector (m, c)
- returns:
  - loss: scalar
  - predictions: prediction vector (m, 1)
"""

def multinomial_lr_test(features, labels, weights):
  loss = -1
  rows = features.shape[0]
  features = append_bias(features)
  m, n, c = features.shape[0], features.shape[1], len(set(labels))
  z = h_x(features,weights)
  activations = soft_max(z)
  loss = ce_loss(labels,activations)
  predictions = prediction(activations)
  return loss, predictions

"""## Gender Recognition

### Hyper-Parameter Tuning
Validating with different combinations of alpha and n_epoch

#### Tuning alpha
"""

# Hyper-parameter tuning
alphas = [1e-3, 1e-2, 1e-1, 1e-0]
# alphas = np.logspace(-4, 1, 100)
parameters = {"f1score":[],"train":[], "eval": [], "best_alpha_loss": alphas[0], "best_alpha_f1": alphas[0], "best_n_epoch": None, "train_loss": None, "weights": None, "predictions": None}
print("Tuning Alpha")
for alpha in tqdm(alphas):
  print("Training for alpha {}".format(alpha))
  epochs = 500
  train_loss, model_weights = multinomial_lr_fit(Train_data_GR, labels_Train_data_GR, epochs, alpha) # Fitting model to training data
  eval_loss, predictions = multinomial_lr_test(Valid_data_GR, labels_Valid_data_GR, model_weights) # Evaluating trained model on validation data
  report = classification_report(labels_Valid_data_GR, predictions, output_dict = True)
  macroF1 = report['macro avg']['f1-score']
  parameters["train"].append(train_loss[-1])
  parameters["eval"].append(eval_loss)
  parameters["f1score"].append(macroF1)
  parameters["best_alpha_loss"] = alpha if min(min(parameters["eval"]), eval_loss) == eval_loss else parameters["best_alpha_loss"]
  parameters["best_alpha_f1"] = alpha if max(max(parameters["f1score"]), macroF1) == macroF1 else parameters["best_alpha_f1"]
  # parameters['weights'][alpha] = model_weights

  print("Loss @ train: {}, eval: {}, f1-score: {}".format(train_loss[-1], eval_loss, macroF1))

# print("Minimum evaluation loss at alpha {}".format(parameters["best_alpha"]))

plt.plot(alphas, parameters["train"], label="Train")
plt.plot(alphas, parameters["eval"], label="Eval")
plt.legend()
plt.title("Final loss over range of alphas")
plt.show()

plt.plot(alphas, parameters["f1score"], label= "f1-score")
plt.legend()
plt.title("F1-score with alphas")
plt.show()

print("Highest Validation F1 score at : ", parameters['best_alpha_f1'])
print("Lowest Validation Loss at : ", parameters['best_alpha_loss'])

"""#### Tuning n_epoch"""

epochs = [epoch for epoch in range(100, 400, 100)]
parameters["train"] = []
parameters["eval"] = []
parameters["best_n_epoch"] = epochs[0]
for n_epoch in tqdm(epochs):
  print("Training for n_epoch {}".format(n_epoch) )
  alpha = parameters["best_alpha_f1"]
  train_loss, model_weights = multinomial_lr_fit(Train_data_GR, labels_Train_data_GR, n_epoch, alpha) # Fitting model to training data
  eval_loss, predictions = multinomial_lr_test(Valid_data_GR, labels_Valid_data_GR, model_weights) # Evaluating trained model on validation data
  parameters["train"].append(train_loss[-1])
  parameters["eval"].append(eval_loss)
  parameters["best_n_epoch"] = n_epoch if min(min(parameters["eval"]), eval_loss) == eval_loss else parameters["best_n_epoch"]
  if parameters["best_n_epoch"] == n_epoch: 
    parameters["train_loss"] = train_loss
    parameters["weights"] = model_weights
    parameters["predictions"] = predictions
    
  print("Loss @ train: {}, eval: {}".format(train_loss[-1], eval_loss))

print("Minimum evaluation loss at n_epoch {}".format(parameters["best_n_epoch"]))

plt.plot(epochs, parameters["train"], label="Train")
plt.plot(epochs, parameters["eval"], label="Eval")
plt.legend()
plt.title("Final loss over range of epochs")

"""### Evaluation"""

train_loss, model_weights = multinomial_lr_fit(Train_data_GR, labels_Train_data_GR, parameters['best_n_epoch'], parameters['best_alpha_f1'])
eval_loss, predictions = multinomial_lr_test(Test_data_GR, labels_Test_data_GR, model_weights)
print("Training Loss:", train_loss[-1])
print("Evaluation Loss:", eval_loss)

plt.plot(train_loss)
plt.title("Final Training Loss")

print("Report on test data")
print(classification_report(labels_Test_data_GR, predictions))
print(confusion_matrix(labels_Test_data_GR, predictions))

"""## Speaker Recognition

### Hyper-Parameter Tuning
Validating with different combinations of alpha and n_epoch

#### Tuning alpha
"""

# Hyper-parameter tuning
alphas = [0.01, 0.1, 0.5]
parameters = {"f1score":[],"train":[], "eval": [], "best_alpha_loss": alphas[0], "best_alpha_f1": alphas[0], "best_n_epoch": None, "train_loss": None, "weights": None, "predictions": None}
print("Tuning Alpha")
for alpha in tqdm(alphas):
  print("Training for alpha {}".format(alpha))
  epochs = 50
  train_loss, model_weights = multinomial_lr_fit(Train_data_SR, labels_Train_data_SR, epochs, alpha) # Fitting model to training data
  eval_loss, predictions = multinomial_lr_test(Valid_data_SR, labels_Valid_data_SR, model_weights) # Evaluating trained model on validation data
  report = classification_report(labels_Valid_data_SR, predictions, output_dict = True)
  macroF1 = report['macro avg']['f1-score']
  parameters["train"].append(train_loss[-1])
  parameters["eval"].append(eval_loss)
  parameters["f1score"].append(macroF1)
  parameters["best_alpha_loss"] = alpha if min(min(parameters["eval"]), eval_loss) == eval_loss else parameters["best_alpha_loss"]
  parameters["best_alpha_f1"] = alpha if max(max(parameters["f1score"]), macroF1) == macroF1 else parameters["best_alpha_f1"]
  # parameters['weights'][alpha] = model_weights

  print("Loss @ train: {}, eval: {}, f1-score: {}".format(train_loss[-1], eval_loss, macroF1))

# print("Minimum evaluation loss at alpha {}".format(parameters["best_alpha"]))

plt.plot(alphas, parameters["train"], label="Train")
plt.plot(alphas, parameters["eval"], label="Eval")
plt.legend()
plt.title("Final loss over range of alphas")
plt.show()

plt.plot(alphas, parameters["f1score"], label= "f1-score")
plt.legend()
plt.title("F1-score with alphas")
plt.show()

print("Highest Validation F1 score at : ", parameters['best_alpha_f1'])
print("Lowest Validation Loss at : ", parameters['best_alpha_loss'])
print("Due to smaller delta in F1 but steep decrease in loss, we chose a higher value of alpha={}".format(0.25))

parameters["best_alpha_f1"] = 0.25

"""#### Tuning n_epoch"""

epochs = [epoch for epoch in range(25, 150, 50)]
parameters["train"] = []
parameters["eval"] = []
parameters["best_n_epoch"] = epochs[0]
for n_epoch in tqdm(epochs):
  print("Training for n_epoch {}".format(n_epoch) )
  alpha = parameters["best_alpha_f1"]
  train_loss, model_weights = multinomial_lr_fit(Train_data_SR, labels_Train_data_SR, n_epoch, alpha) # Fitting model to training data
  eval_loss, predictions = multinomial_lr_test(Valid_data_SR, labels_Valid_data_SR, model_weights) # Evaluating trained model on validation data
  parameters["train"].append(train_loss[-1])
  parameters["eval"].append(eval_loss)
  parameters["best_n_epoch"] = n_epoch if min(min(parameters["eval"]), eval_loss) == eval_loss else parameters["best_n_epoch"]
  if parameters["best_n_epoch"] == n_epoch: 
    parameters["train_loss"] = train_loss
    parameters["weights"] = model_weights
    parameters["predictions"] = predictions
    
  print("Loss @ train: {}, eval: {}".format(train_loss[-1], eval_loss))

print("Minimum evaluation loss at n_epoch {}".format(parameters["best_n_epoch"]))

plt.plot(epochs, parameters["train"], label="Train")
plt.plot(epochs, parameters["eval"], label="Eval")
plt.legend()
plt.title("Final loss over range of epochs")

"""### Evaluation"""

train_loss, model_weights = multinomial_lr_fit(Train_data_SR, labels_Train_data_SR, parameters['best_n_epoch'], parameters['best_alpha_f1'])
eval_loss, predictions = multinomial_lr_test(Test_data_SR, labels_Test_data_SR, model_weights)
print("Training Loss:", train_loss[-1])
print("Evaluation Loss:", eval_loss)

plt.plot(train_loss)
plt.title("Final Training Loss")

print("================= {} ==================".format("Test Report"))
print(classification_report(labels_Test_data_SR, predictions))
print("================= {} ==================".format("Confusion Matrix"))
print(confusion_matrix(labels_Test_data_SR, predictions))

